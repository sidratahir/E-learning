Christopher	Manning Pandu	Nayak
Introduction	to Information	Retrieval
CS276:	Information	Retrieval	and	Web	Search Lecture	19:	Web	Question	Answering


“Information	retrieval”
The	name	information	retrieval	is	standard,	but	as	traditionally	practiced,	it’s	not	really	right
All	you	get	is	document	retrieval,	and	beyond	that	the	job	is	up	to	you.

Getting	information The common person’s view? [From a novel] “I like the Internet. Really, I do. Any time I need a piece of shareware or I want to find out the weather in Bogota … I’m the first guy to get the modem humming.  But as a source of information, it sucks.  You got a billion pieces of data, struggling to be heard and seen and downloaded, and anything I want to know seems to get trampled underfoot in the crowd.”
Web	Search	in	2025? The	web,	it	is	a	changing.	
What	will	people	do	in	2025? § Type	key	words	into	a	search	box? § Use	the	Semantic	Web? § Ask	questions	to	their	computer	in	natural	language? § Use	social	or	“human	powered”	search?
What	do	we	know	that’s	happening? § Much	of	what	is	going	on	is	in	the	products	of	companies,	and	there	isn’t	exactly	careful	research	explaining	or	evaluating	it § So	most	of	this	is	my	own	meandering	observations	giving	voice	over	to	slides	from	others
Google What’s	been	happening?	2013–2017 § Many	updates	a	year	…	and	3rd	party	sites	try	to	track	them § e.g.,	https://moz.com/google-algorithm-changeby	&	aimed	at	SEOs § I	just	mention	a	few	changes	here
§ New	search	index	at	Google:	“Hummingbird”	(2013) § http://www.forbes.com/sites/roberthof/2013/09/26/google-just-revampedsearch-to-handle-your-long-questions/ § Answering	long,	“natural	language”	questions	better § Partly	to	deal	with	spoken	queries	on	mobile § More	use	of	the	Google	Knowledge	Graph	(2014) § Concepts	versus	words § RankBrain(second	half	of	2015): § A	neural	net	helps	in	document	matching	for	the	long	tail
Google What’s	been	happening?	2013–2017 § “Pigeon”	update	(July	2014): § More	use	of	distance	and	location	in	ranking	signals § “Mobilegeddon”	(Apr	21,	2015): § “Mobile	friendliness”	as	a	major	ranking	signal § “App	Indexing”	(Android,	iOS	support	May	2015) § Search	results	can	take	you	to	an	app § Mobile-friendly	2	(May	12,	2016): § About	half	of	all	searches	are	now	from	mobile § “Fred”	(1st	quarter	2017) § Various	changes	discounting	spammy,	clickbaity,	fake?	sites
The	role	of	knowledge	bases § Google	Knowledge	Graph § Facebook	Graph	Search § Bing’s	Satori § Things	like	Wolfram	Alpha
Common	theme:	Doing	graph	search	over	structured	knowledge	rather	than	traditional	text	search
Introduction	to	Information	Retrieval What’s	been	happening § More	semi-structured	information	embedded	in	web	pages § schema.org
Introduction	to	Information	Retrieval Mobile Move	to	mobile	favors	a	move	to	speech	which	favors	natural	language	information	search
§ Will	we	move	to	a	time	when	over	half	of	searches	are	spoken?
Introduction	to	Information	Retrieval Mobile § Mobile	proved	importance	of	NLU/QA § [What	is	the	best	time	for	wildflowers	in	the	bay	area]
Introduction	to	Information	Retrieval Information	quality § There	have	always	been	concerns	about	information	provenance(the	source)	and	information	reliability,	especially	among	“information	professionals”	(reporters,	lawyers,	spies,	…) § It	wasn’t	ignored	on	the	web:	ideas	like	PageRank	were	meant	to	find	good	content,	and	there	has	been	a	decade	of	work	targeting	link	farms,	etc.	§ However,	a	lot	of	recent	events	have	shown	the	limited	effectiveness	of	that	work,	and	how	“fake”	information	easily	gets	upvotedand	spreads
Introduction	to	Information	Retrieval Towards	intelligent	agents Two	goals § Things	not	strings § Inference	not	search
Introduction	to	Information	Retrieval Two	paradigms	for	question	answering	§ Text-based	approaches § TREC	QA,	IBM	Watson,	DrQA § Structured	knowledge-based	approaches § Apple	Siri,	Wolfram	Alpha,	Facebook	Graph	Search
(And,	of	course,	there	are	hybrids,	including	some	of	the	above.)
At	the	moment,	structured	knowledge	is	back	in	fashion,	but	it	may	or	may	not	last
Introduction	to	Information	Retrieval Example	from	Fernando	Pereira	(GOOG)
Introduction	to	Information	Retrieval
Introduction	to	Information	Retrieval
Introduction	to	Information	Retrieval
Introduction	to	Information	Retrieval
Introduction	to	Information	Retrieval
Introduction	to	Information	Retrieval
Introduction	to	Information	Retrieval
Introduction	to	Information	Retrieval
Introduction	to	Information	Retrieval Slides	from	Patrick	Pantel(MSFT)
Introduction	to	Information	Retrieval
Introduction	to	Information	Retrieval
Introduction	to	Information	Retrieval
Direct	Answer
Structured	Data
Introduction	to	Information	Retrieval
Introduction	to	Information	Retrieval
Introduction	to	Information	Retrieval Patrick	Panteltalk (Then)	Current	experience
Introduction	to	Information	Retrieval Desired	experience:	Towards	actions
Introduction	to	Information	Retrieval Politician
Introduction	to	Information	Retrieval Actions	vs.	Intents
Introduction	to	Information	Retrieval Learning	actions	from	web	usage	logs
Introduction	to	Information	Retrieval
Introduction	to	Information	Retrieval
Introduction	to	Information	Retrieval
Introduction	to	Information	Retrieval
Introduction	to	Information	Retrieval Entity	disambiguation	and	linking § Key	requirement	is	that	entities	get	identified § Named	entity	recognition	(e.g.,	Stanford	NER!) § and	disambiguated § Entity	linking	(or	sometimes	“Wikification”) § e.g.,	Michael	Jordan	the	basketballeror	the	ML	guy
Introduction	to	Information	Retrieval
Sergio talkedto Ennio about Eli‘srolein the Ecstasy scene. Thissequenceon thegraveyard was a highlightin Sergio‘strilogy ofwestern films. Mentions,	Meanings,	Mappings			[G.	Weikum]
D5 Overview   May 30, 2011
Sergio means Sergio_Leone Sergio means Serge_Gainsbourg Ennio  means Ennio_Antonelli Ennio  means Ennio_Morricone Eli  means Eli_(bible) Eli  means ExtremeLightInfrastructure Eli  means Eli_Wallach Ecstasy  means Ecstasy_(drug) Ecstasy  means Ecstasy_of_Gold trilogy means Star_Wars_Trilogy trilogy means Lord_of_the_Rings trilogy means Dollars_Trilogy … … … KB
Eli (bible) Eli Wallach
Mentions (surface names)
Entities (meanings) Dollars Trilogy Lord oftheRings Star Wars Trilogy Benny Andersson Benny Goodman Ecstasy ofGold Ecstasy (drug)?
Introduction	to	Information	Retrieval
§ and	linked	to	a	canonical	reference § Freebase,	dbPedia,	Yago2,	(WordNet)
Introduction	to	Information	Retrieval Understanding	questions
Introduction	to	Information	Retrieval
Introduction	to	Information	Retrieval
Introduction	to	Information	Retrieval 3	approaches	to	question	answering: Knowledge-based	approaches	(Siri)
§ Build	a	semantic	representation	of	the	query § Times,	dates,	locations,	entities,	numeric	quantities § Map	from	this	semantics	to	query	structured	data	or	resources § Geospatial	databases § Ontologies	(Wikipedia	infoboxes,	dbPedia,	WordNet,	Yago) § Restaurant	review	sources	and	reservation	services § Scientific	databases § Wolfram	Alpha
45
Introduction	to	Information	Retrieval Text-based	(mainly	factoid)	QA § QUESTION	PROCESSING § Detect	question	type,	answer	type,	focus,	relations § Formulate	queries	to	send	to	a	search	engine § PASSAGE	RETRIEVAL § Retrieve	ranked	documents § Break	into	suitable	passages	and	rerank § ANSWER	PROCESSING § Extract	candidate	answers	(as	named	entities) § Rank	candidates	§ using	evidence	from	relations	in	the	text	and	external	sources
Introduction	to	Information	Retrieval Hybrid	approaches	(IBM	Watson) § Build	a	shallow	semantic	representation	of	the	query § Generate	answer	candidates	using	IR	methods § Augmented	with	ontologies	and	semi-structured	data § Score	each	candidate	using	richer	knowledge	sources § Geospatial	databases § Temporal	reasoning § Taxonomical	classification
47
Texts	are	Knowledge
Knowledge:	Jeremy	Zawodnysays	…
Is	the	goal	to	go	from	language	to	knowledge	bases?	• For	humans,	going	from	the	largely	unstructured	language	on	the	web	to	actionable	information	is	effortlessly	easy • But	for	computers,	it’s	rather	difficult! • This	has	suggested	to	many	that	if	we’re	going	to	produce	the	next	generation	of	intelligent	agents,	which	can	make	decisions	on	our	behalf • Answering	our	routine	email • Booking	our	next	trip	to	Fiji then	we	still	first	need	to	construct	knowledge	bases • To	go	from	languages	to	information • But	should	we	rather	just	have	computers	work	with	language?
Knowledge: Not	just	semantics	but	pragmatics
Pragmatics	=	taking	account	of	context	in	determining	meaning A	natural	part	of	language	understanding	and	use
Search	engines	are	great	because	they	inherently	take	into	account	pragmatics	(“associations	and	contexts”) • [the	national] à The	National		(a	band) • [the	national	ohio] à The	National	-BloodbuzzOhio	–YouTube • [the	national	broadband] à www.broadband.gov
Lemmon was awarded the Best Supporting Actor Oscar in 1956 for Mister Roberts (1955) and the Best Actor Oscar for Save the Tiger (1973), becoming the first actor to achieve this rare double… Source: Jack Lemmon -- Wikipedia Who won the best actor Oscar in 1973?
Scott Wen-tau Yih (ACL 2013) paper
Assume that there is an underlying alignment Describes which words in  and  can be associated
What is the fastest car in the world?
The Jaguar XJ220 is the dearest, fastest and most sought after car on the planet. 
See if the (syntactic/semantic) relations support the answer
[Harabagiu & Moldovan, 2001]
Introduction	to	Information	Retrieval Full	NLP	QA:	LCC	(Harabagiu/Moldovan)	[below	is	the	architecture	of	LCC’s QA	system	circa	2003]
Question Parse
Semantic Transformation
Recognition of Expected Answer Type (for NER)
Keyword Extraction
Factoid Question
List Question
Named Entity Recognition (CICERO LITE)
Answer Type Hierarchy (WordNet)
Question Processing
Question Parse
Pattern Matching
Keyword Extraction
Question Processing
Definition Question Definition Answer Answer Extraction Pattern Matching Definition Answer Processing
Answer Extraction
Threshold Cutoff
List Answer Processing List Answer
Answer Extraction (NER)
Answer Justification (alignment, relations)
Answer Reranking (~ Theorem Prover)
Factoid Answer Processing
Axiomatic Knowledge Base
Factoid Answer
Multiple Definition Passages
Pattern Repository
Single Factoid Passages
Multiple List Passages
Passage Retrieval
Document Processing
Document Index
Document Collection
DrQA: Open-domain Question Answering (Chen, et al. ACL 2017) https://arxiv.org/abs/1704.00051
55
WebQuestions(Berant et al, 2013) Q: What part of the atom did Chadwick discover?  A:neutron
TREC Q: What U.S. state’s motto is “Live free or Die”?  A:New Hampshire
WikiMovies (Miller et al, 2016) Q: Who wrote the film Gigli?  A:Martin Brest
SQuAD Q: How many of Warsaw's inhabitants spoke Polish in 1933? A: 833,500
56
Open-domain Question Answering
Document Reader
Document Retriever 833,500
Q:  How many of Warsaw's inhabitants spoke Polish in 1933?
57
Document Retriever
70-86% of questions we have that the answer segment appears in the top 5 articles
(Chen et al, 2017) 
58
Traditional tf.idf inverted index + efficient bigram hash
59
Document Reader: Stanford Attentive Reader characters in " @placeholder" movies have gradually become more diverse Q Bidirectional LSTMs (Chen et al, 2016) 
characters in “ @placeholder more diverse …
…
…
60
Stanford Attentive Reader Q
… … …P
Bidirectional LSTMs
(Chen et al, 2016) 
entity6A
characters in " @placeholder" movies have gradually become more diverse
Attention
61
Stanford Attentive Reader++ Who did Genghis Khanunitebeforehe began conqueringthe rest of Eurasia?Q
… … …P
Bidirectional RNNs
Attention
predict starttoken
Attention
predict endtoken
62
Results (single model)
F1 Logistic regression 51.0 Fine-Grained Gating (Carnegie Mellon U) 73.3 Match-LSTM (Singapore Management U) 73.7 DCN (Salesforce) 75.9 BiDAF (UW & Allen Institute) 77.3 Multi-Perspective Matching (IBM) 78.7 ReasoNet (MSR Redmond) 79.4 DrQA (Chen et al. 2017) 79.4 r-net (MSR Asia) 80.8 Human performance 91.2
(Chen et al, 2017) 
63
27
20
12
25
28 26
20
34
30
25
21
37
0 10 20 30 40
SQuAD WebQuestions
Pre-trained SQuAD Fine-tuning Multi-task learning
Exact match (top-1 prediction)
Results
(Chen et al, 2017) 
64
Demo
65
Demo
66
Demo
Introduction	to	Information	Retrieval Thanks everyone! ?