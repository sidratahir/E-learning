Introduc)on?to? Informa(on?Retrieval?
CS276:?Informa)on?Retrieval?and?Web?Search? Pandu?Nayak?and?Prabhakar?Raghavan?
Lecture?5:?Index?Compression?
Introduc)on to Informa)on Retrieval 
Course?work? ?? Problem?set?1?due?Thursday? ?? Programming?exercise?1?will?be?handed?out?today?
2?
Introduc)on to Informa)on Retrieval 
Last?lecture?–?index?construc)on? ?? Sort-based?indexing? ?? Naïve?in-memory?inversion? ?? Blocked?Sort-Based?Indexing? ?? Merge?sort?is?e?ec)ve?for?disk-based?sor)ng?(avoid?seeks!)? ?? Single-Pass?In-Memory?Indexing? ?? No?global?dic)onary? ?? Generate?separate?dic)onary?for?each?block? ?? Don’t?sort?pos)ngs? ?? Accumulate?pos)ngs?in?pos)ngs?lists?as?they?occur? ?? Distributed?indexing?using?MapReduce? ?? Dynamic?indexing:?Mul)ple?indices,?logarithmic?merge? 3?
Introduc)on to Informa)on Retrieval 
Today?
?? Collec)on?sta)s)cs?in?more?detail?(with?RCV1)? ??How?big?will?the?dic)onary?and?pos)ngs?be?? ?? Dic)onary?compression? ?? Pos)ngs?compression?
Ch. 5 
4?
Introduc)on to Informa)on Retrieval 
Why?compression?(in?general)?? ?? Use?less?disk?space? ?? Saves?a?li]le?money? ?? Keep?more?stu??in?memory? ?? Increases?speed? ?? Increase?speed?of?data?transfer?from?disk?to?memory? ?? [read?compressed?data?|?decompress]?is?faster?than????? [read?uncompressed?data]? ?? Premise:?Decompression?algorithms?are?fast? ?? True?of?the?decompression?algorithms?we?use?
Ch. 5 
5?
Introduc)on to Informa)on Retrieval 
Why?compression?for?inverted?indexes?? ?? Dic)onary? ?? Make?it?small?enough?to?keep?in?main?memory? ?? Make?it?so?small?that?you?can?keep?some?pos)ngs?lists?in? main?memory?too? ?? Pos)ngs??le(s)? ?? Reduce?disk?space?needed? ?? Decrease?)me?needed?to?read?pos)ngs?lists?from?disk? ?? Large?search?engines?keep?a?signi?cant?part?of?the?pos)ngs? in?memory.? ?? Compression?lets?you?keep?more?in?memory? ?? We?will?devise?various?IR-speci?c?compression?schemes?
Ch. 5 
6?
Introduc)on to Informa)on Retrieval 
Recall?Reuters?RCV1? ?? symbol ?sta(s(c?? ?????value? ?? N? ? ??documents ? ??? ? ?800,000? ?? L? ? ??avg.?#?tokens?per?doc?? ?200? ?? M ? ??terms?(=?word?types)?? ?~400,000? ?? ???????????????? ?avg.?#?bytes?per?token ? ?6? ??????????????????? ?(incl.?spaces/punct.)? ?? ???????????????? ?avg.?#?bytes?per?token ? ?4.5? ???????????? ?(without?spaces/punct.)? ?? ???????????????? ?avg.?#?bytes?per?term? ?7.5? ?? ???????????????? ?non-posi)onal?pos)ngs ?100,000,000?
Sec. 5.1 
7?
Introduc)on to Informa)on Retrieval  Index?parameters?vs.?what?we?index? (details?IIR Table?5.1,?p.80)?
size of word types (terms) non-positional postings 
positional postings 
dictionary non-positional index  positional index Size (K) ?% cumul % Size (K) ? % cumul % Size (K) ? % cumul % Unfiltered 484 109,971 197,879 No numbers 474 -2 -2 100,680 -8 -8 179,158 -9 -9 Case folding 392 -17 -19 96,969 -3 -12 179,158 0 -9 30 stopwords 391 -0 -19 83,390 -14 -24 121,858 -31 -38 150 stopwords 391 -0 -19 67,002 -30 -39 94,517 -47 -52 stemming 322 -17 -33 63,812 -4 -42 94,517 0 -52 Exercise: give intuitions for all the ‘0’ entries. Why do some zero entries correspond to big deltas in other columns?  
Sec. 5.1 
8?
Introduc)on to Informa)on Retrieval 
Lossless?vs.?lossy?compression? ?? Lossless?compression:?All?informa)on?is?preserved.? ?? What?we?mostly?do?in?IR.? ?? Lossy?compression:?Discard?some?informa)on? ?? Several?of?the?preprocessing?steps?can?be?viewed?as? lossy?compression:?case?folding,?stop?words,? stemming,?number?elimina)on.? ?? Chap/Lecture?7:?Prune?pos)ngs?entries?that?are? unlikely?to?turn?up?in?the?top?k?list?for?any?query.? ?? Almost?no?loss?quality?for?top?k?list.?
Sec. 5.1 
9?
Introduc)on to Informa)on Retrieval 
Vocabulary?vs.?collec)on?size? ?? How?big?is?the?term?vocabulary?? ?? That?is,?how?many?dis)nct?words?are?there?? ?? Can?we?assume?an?upper?bound?? ?? Not?really:?At?least?7020?=?1037?di?erent?words?of?length?20? ?? In?prac)ce,?the?vocabulary?will?keep?growing?with?the? collec)on?size? ?? Especially?with?Unicode???
Sec. 5.1 
10?
Introduc)on to Informa)on Retrieval 
Vocabulary?vs.?collec)on?size? ?? Heaps’?law:?M = kTb  ?? M?is?the?size?of?the?vocabulary,?T?is?the?number?of? tokens?in?the?collec)on? ?? Typical?values:?30?=?k?=?100?and?b?˜?0.5? ?? In?a?log-log?plot?of?vocabulary?size?M?vs.?T,?Heaps’? law?predicts?a?line?with?slope?about?½? ?? It?is?the?simplest?possible?rela)onship?between?the?two?in? log-log?space? ?? An?empirical??nding?(“empirical?law”)?
Sec. 5.1 
11?
Introduc)on to Informa)on Retrieval 
Heaps’?Law? For?RCV1,?the?dashed?line? log10M?=?0.49?log10T?+?1.64? is?the?best?least?squares??t.? Thus,?M?=?101.64T0.49?so?k?=? 101.64?˜?44?and?b?=?0.49.?
Good?empirical??t?for? Reuters?RCV1?!?
For??rst?1,000,020?tokens,? law?predicts?38,323?terms;? actually,?38,365?terms?
Fig 5.1 p81 
Sec. 5.1 
12?
Introduc)on to Informa)on Retrieval 
Exercises? ?? What?is?the?e?ect?of?including?spelling?errors,?vs.? automa)cally?correc)ng?spelling?errors?on?Heaps’? law?? ?? Compute?the?vocabulary?size?M for?this?scenario:  ?? Looking?at?a?collec)on?of?web?pages,?you??nd?that?there? are?3000?di?erent?terms?in?the??rst?10,000?tokens?and? 30,000?di?erent?terms?in?the??rst?1,000,000?tokens.? ?? Assume?a?search?engine?indexes?a?total?of?20,000,000,000? (2?×?1010)?pages,?containing?200?tokens?on?average? ?? What?is?the?size?of?the?vocabulary?of?the?indexed?collec)on? as?predicted?by?Heaps’?law??
Sec. 5.1 
13?
Introduc)on to Informa)on Retrieval 
Zipf’s?law? ?? Heaps’?law?gives?the?vocabulary?size?in?collec)ons.? ?? We?also?study?the?rela)ve?frequencies?of?terms.? ?? In?natural?language,?there?are?a?few?very?frequent? terms?and?very?many?very?rare?terms.? ?? Zipf’s?law:?The?ith?most?frequent?term?has?frequency? propor)onal?to?1/i?.? ?? cfi???1/i = K/i where?K?is?a?normalizing?constant  ?? cfi?is?collec)on?frequency:?the?number?of? occurrences?of?the?term?ti?in?the?collec)on.?
Sec. 5.1 
14?
Introduc)on to Informa)on Retrieval 
Zipf?consequences? ?? If?the?most?frequent?term?(the)?occurs?cf1?)mes?? ?? then?the?second?most?frequent?term?(of)?occurs?cf1/2?)mes? ?? the?third?most?frequent?term?(and)?occurs?cf1/3?)mes?…?? ?? Equivalent:?cfi?=?K/i?where?K?is?a?normalizing?factor,? so? ??log?cfi?=?log?K?-?log?i  ??Linear?rela)onship?between?log?cfi?and?log?i 
??Another?power?law?rela)onship?
Sec. 5.1 
15?
Introduc)on to Informa)on Retrieval 
Zipf’s?law?for?Reuters?RCV1?
16?
Sec. 5.1 
Introduc)on to Informa)on Retrieval 
Compression? ??Now,?we?will?consider?compressing?the?space? for?the?dic)onary?and?pos)ngs? ??Basic?Boolean?index?only? ??No?study?of?posi)onal?indexes,?etc.? ??We?will?consider?compression?schemes?
Ch. 5 
17?
Introduc)on to Informa)on Retrieval 
DICTIONARY?COMPRESSION?
Sec. 5.2 
18?
Introduc)on to Informa)on Retrieval 
Why?compress?the?dic)onary?? ?? Search?begins?with?the?dic)onary? ?? We?want?to?keep?it?in?memory? ?? Memory?footprint?compe))on?with?other? applica)ons? ?? Embedded/mobile?devices?may?have?very?li]le? memory? ?? Even?if?the?dic)onary?isn’t?in?memory,?we?want?it?to? be?small?for?a?fast?search?startup?)me? ?? So,?compressing?the?dic)onary?is?important?
Sec. 5.2 
19?
Introduc)on to Informa)on Retrieval 
Dic)onary?storage?-??rst?cut? ?? Array?of??xed-width?entries? ?? ~400,000?terms;?28?bytes/term?=?11.2?MB.?
Dictionary search structure 
20 bytes 4 bytes each 
Sec. 5.2 
20?
Introduc)on to Informa)on Retrieval 
Fixed-width?terms?are?wasteful? ?? Most?of?the?bytes?in?the?Term?column?are?wasted?–? we?allot?20?bytes?for?1?le]er?terms.? ?? And?we?s)ll?can’t?handle?supercalifragilis)cexpialidocious or? hydrochloro?uorocarbons.  ?? Wri]en?English?averages?~4.5?characters/word.? ?? Exercise:?Why?is/isn’t?this?the?number?to?use?for?es)ma)ng? the?dic)onary?size?? ?? Ave.?dic)onary?word?in?English:?~8?characters? ?? How?do?we?use?~8?characters?per?dic)onary?term?? ?? Short?words?dominate?token?counts?but?not?type? average.?
Sec. 5.2 
21?
Introduc)on to Informa)on Retrieval  Compressing?the?term?list:?? Dic)onary-as-a-String?
….systilesyzygeticsyzygialsyzygyszaibelyiteszczecinszomo…. 
Total string length = 400K x 8B = 3.2MB 
Pointers resolve 3.2M positions: log23.2M = 22bits = 3bytes 
??Store dictionary as a (long) string of characters: ??Pointer to next word shows end of current word ??Hope to save up to 60% of dictionary space. 
Sec. 5.2 
22?
Introduc)on to Informa)on Retrieval 
Space?for?dic)onary?as?a?string? ?? 4?bytes?per?term?for?Freq.? ?? 4?bytes?per?term?for?pointer?to?Pos)ngs.? ?? 3?bytes?per?term?pointer? ?? Avg.?8?bytes?per?term?in?term?string? ?? 400K?terms?x?19???7.6?MB?(against?11.2MB?for??xed? width)? ? Now avg. 11 ? bytes/term, ? not 20. 
Sec. 5.2 
23?
Introduc)on to Informa)on Retrieval 
Blocking? ?? Store?pointers?to?every?kth?term?string.? ?? Example?below:?k=4.? ?? Need?to?store?term?lengths?(1?extra?byte)? ….7systile9syzygetic8syzygial6syzygy11szaibelyite8szczecin9szomo…. 
? Save 9 bytes ? on 3 ? pointers. 
Lose 4 bytes on term lengths. 
Sec. 5.2 
24?
Introduc)on to Informa)on Retrieval 
Net? ?? Example?for?block?size?k?=?4? ?? Where?we?used?3?bytes/pointer?without?blocking? ?? 3?x?4?=?12?bytes,? now?we?use?3?+?4?=?7?bytes.?
Shaved?another?~0.5MB.?This?reduces?the?size?of?the? dic)onary?from?7.6?MB?to?7.1?MB.? We?can?save?more?with?larger?k.?
Why not go with larger k? 
Sec. 5.2 
25?
Introduc)on to Informa)on Retrieval 
Exercise? ?? Es)mate?the?space?usage?(and?savings?compared?to? 7.6?MB)?with?blocking,?for?block?sizes?of?k = 4, 8 and  16. 
Sec. 5.2 
26?
Introduc)on to Informa)on Retrieval 
Dic)onary?search?without?blocking? ?? Assuming?each? dic)onary?term?equally? likely?in?query?(not?really? so?in?prac)ce!),?average? number?of?comparisons? =?(1+2·2+4·3+4)/8?~2.6?
Sec. 5.2 
Exercise:?what?if?the?frequencies? of?query?terms?were?non-uniform? but?known,?how?would?you? structure?the?dic)onary?search? tree??
27?
Introduc)on to Informa)on Retrieval 
Dic)onary?search?with?blocking?
?? Binary?search?down?to?4-term?block;? ?? Then?linear?search?through?terms?in?block.? ?? Blocks?of?4?(binary?tree),?avg.?=? (1+2·2+2·3+2·4+5)/8?=?3?compares?
Sec. 5.2 
28?
Introduc)on to Informa)on Retrieval 
Exercise? ?? Es)mate?the?impact?on?search?performance?(and? slowdown?compared?to?k=1)?with?blocking,?for?block? sizes?of?k = 4, 8 and 16. 
Sec. 5.2 
29?
Introduc)on to Informa)on Retrieval 
Front?coding? ?? Front-coding:? ?? Sorted?words?commonly?have?long?common?pre?x?–?store? di?erences?only? ?? (for?last?k-1?in?a?block?of?k)? 8automata8automate9automa’c10automa’on 
?8automat*a1?e2?ic3?ion 
Encodes automat Extra length beyond automat. Begins to resemble general string compression. 
Sec. 5.2 
30?
Introduc)on to Informa)on Retrieval 
RCV1?dic)onary?compression?summary?
Technique? Size?in?MB?
Fixed?width? 11.2?
Dic)onary-as-String?with?pointers?to?every?term? 7.6?
Also,?blocking?k =?4? 7.1?
Also,?Blocking?+?front?coding? 5.9?
Sec. 5.2 
31?
Introduc)on to Informa)on Retrieval 
POSTINGS?COMPRESSION?
Sec. 5.3 
32?
Introduc)on to Informa)on Retrieval 
Pos)ngs?compression? ?? The?pos)ngs??le?is?much?larger?than?the?dic)onary,? factor?of?at?least?10.? ?? Key?desideratum:?store?each?pos)ng?compactly.? ?? A?pos)ng?for?our?purposes?is?a?docID.? ?? For?Reuters?(800,000?documents),?we?would?use?32? bits?per?docID?when?using?4-byte?integers.? ?? Alterna)vely,?we?can?use?log2?800,000?˜?20?bits?per? docID.? ?? Our?goal:?use?far?fewer?than?20?bits?per?docID.?
Sec. 5.3 
33?
Introduc)on to Informa)on Retrieval 
Pos)ngs:?two?con?ic)ng?forces? ?? A?term?like?arachnocentric occurs?in?maybe?one?doc? out?of?a?million?–?we?would?like?to?store?this?pos)ng? using?log2?1M?~?20?bits.? ?? A?term?like?the?occurs?in?virtually?every?doc,?so?20? bits/pos)ng?is?too?expensive.? ?? Prefer?0/1?bitmap?vector?in?this?case??
Sec. 5.3 
34?
Introduc)on to Informa)on Retrieval 
Pos)ngs??le?entry? ?? We?store?the?list?of?docs?containing?a?term?in? increasing?order?of?docID.? ?? computer:?33,47,154,159,202?…? ?? Consequence:?it?su?ces?to?store?gaps.? ?? 33,14,107,5,43?…? ?? Hope:?most?gaps?can?be?encoded/stored?with?far? fewer?than?20?bits.?
Sec. 5.3 
35?
Introduc)on to Informa)on Retrieval 
Three?pos)ngs?entries?
Sec. 5.3 
36?
Introduc)on to Informa)on Retrieval 
Variable?length?encoding? ?? Aim:? ?? For?arachnocentric,?we?will?use?~20?bits/gap?entry.? ?? For?the,?we?will?use?~1?bit/gap?entry.? ?? If?the?average?gap?for?a?term?is?G,?we?want?to?use? ~log2G?bits/gap?entry.? ?? Key?challenge:?encode?every?integer?(gap)?with?about? as?few?bits?as?needed?for?that?integer.? ?? This?requires?a?variable length encoding  ?? Variable?length?codes?achieve?this?by?using?short? codes?for?small?numbers?
Sec. 5.3 
37?
Introduc)on to Informa)on Retrieval 
Variable?Byte?(VB)?codes? ?? For?a?gap?value?G, we?want?to?use?close?to?the?fewest? bytes?needed?to?hold?log2?G?bits? ?? Begin?with?one?byte?to?store?G?and?dedicate?1?bit?in?it? to?be?a?con)nua)on?bit?c? ?? If?G?=127,?binary-encode?it?in?the?7?available?bits?and? set?c =1? ?? Else?encode?G’s?lower-order?7?bits?and?then?use? addi)onal?bytes?to?encode?the?higher?order?bits? using?the?same?algorithm? ?? At?the?end?set?the?con)nua)on?bit?of?the?last?byte?to? 1?(c?=1)?–?and?for?the?other?bytes?c?=?0.?
Sec. 5.3 
38?
Introduc)on to Informa)on Retrieval 
Example?
docIDs 824  829  215406 gaps 5 214577 VB code 00000110 10111000  10000101  00001101 00001100 10110001 Postings stored as the byte concatenation 000001101011100010000101000011010000110010110001 
Key property: VB-encoded postings are uniquely prefix-decodable. 
For a small gap (5), VB uses a whole byte. 
Sec. 5.3 
39?
Introduc)on to Informa)on Retrieval 
Other?variable?unit?codes? ?? Instead?of?bytes,?we?can?also?use?a?di?erent?“unit?of? alignment”:?32?bits?(words),?16?bits,?4?bits?(nibbles).? ?? Variable?byte?alignment?wastes?space?if?you?have? many?small?gaps?–?nibbles?do?be]er?in?such?cases.? ?? Variable?byte?codes:? ?? Used?by?many?commercial/research?systems? ?? Good?low-tech?blend?of?variable-length?coding?and? sensi)vity?to?computer?memory?alignment?matches?(vs.? bit-level?codes,?which?we?look?at?next).? ?? There?is?also?recent?work?on?word-aligned?codes?that? pack?a?variable?number?of?gaps?into?one?word?
Sec. 5.3 
40?
Introduc)on to Informa)on Retrieval 
Unary?code? ?? Represent?n?as?n?1s?with?a??nal?0.? ?? Unary?code?for?3?is?1110.? ?? Unary?code?for?40?is? 11111111111111111111111111111111111111110?.? ?? Unary?code?for?80?is:? 111111111111111111111111111111111111111111 111111111111111111111111111111111111110?
?? This?doesn’t?look?promising,?but….?
41?
Introduc)on to Informa)on Retrieval 
Gamma?codes? ?? We?can?compress?be]er?with?bit-level?codes? ?? The?Gamma?code?is?the?best?known?of?these.? ?? Represent?a?gap?G?as?a?pair?length?and?o?set? ?? o?set?is?G?in?binary,?with?the?leading?bit?cut?o?? ?? For?example?13???1101???101? ?? length?is?the?length?of?o?set? ?? For?13?(o?set?101),?this?is?3.? ?? We?encode?length?with?unary code:?1110.? ?? Gamma?code?of?13?is?the?concatena)on?of?length? and?o?set:?1110101?
Sec. 5.3 
42?
Introduc)on to Informa)on Retrieval 
Gamma?code?examples?
number length  offset  ?-code 0 none 1 0 0 2 10 0 10,0 3 10 1 10,1 4 110  00 110,00 9 1110 001 1110,001 13 1110 101 1110,101 24 11110 1000 11110,1000 511 111111110 11111111 111111110,11111111 1025 11111111110 0000000001 11111111110,0000000001 
Sec. 5.3 
43?
Introduc)on to Informa)on Retrieval 
Gamma?code?proper)es? ?? G?is?encoded?using?2??log?G??+?1?bits? ?? Length?of?o?set?is??log?G??bits? ?? Length?of?length?is??log?G??+?1?bits? ?? All?gamma?codes?have?an?odd?number?of?bits? ?? Almost?within?a?factor?of?2?of?best?possible,?log2?G?
?? Gamma?code?is?uniquely?pre?x-decodable,?like?VB? ?? Gamma?code?can?be?used?for?any?distribu)on? ?? Gamma?code?is?parameter-free?
Sec. 5.3 
44?
Introduc)on to Informa)on Retrieval 
Gamma?seldom?used?in?prac)ce? ?? Machines?have?word?boundaries?–?8,?16,?32,?64?bits? ?? Opera)ons?that?cross?word?boundaries?are?slower? ?? Compressing?and?manipula)ng?at?the?granularity?of? bits?can?be?slow? ?? Variable?byte?encoding?is?aligned?and?thus?poten)ally? more?e?cient? ?? Regardless?of?e?ciency,?variable?byte?is?conceptually? simpler?at?li]le?addi)onal?space?cost?
Sec. 5.3 
45?
Introduc)on to Informa)on Retrieval 
RCV1?compression?
Data structure  Size in MB dictionary, fixed-width 11.2 dictionary, term pointers into string 7.6 with blocking, k = 4 7.1 with blocking & front coding 5.9 collection (text, xml markup etc) 3,600.0 collection (text) 960.0 Term-doc incidence matrix 40,000.0 postings, uncompressed (32-bit words) 400.0 postings, uncompressed (20 bits) 250.0 postings, variable byte encoded 116.0 postings, ?-encoded 101.0 
Sec. 5.3 
46?
Introduc)on to Informa)on Retrieval 
Index?compression?summary? ?? We?can?now?create?an?index?for?highly?e?cient? Boolean?retrieval?that?is?very?space?e?cient? ?? Only?4%?of?the?total?size?of?the?collec)on? ?? Only?10-15%?of?the?total?size?of?the?text?in?the? collec)on? ?? However,?we’ve?ignored?posi)onal?informa)on? ?? Hence,?space?savings?are?less?for?indexes?used?in? prac)ce? ?? But?techniques?substan)ally?the?same.?
Sec. 5.3 
47?
Introduc)on to Informa)on Retrieval 
Resources?for?today’s?lecture? ?? IIR?5? ?? MG?3.3,?3.4.? ?? F.?Scholer,?H.E.?Williams?and?J.?Zobel.?2002.? Compression?of?Inverted?Indexes?For?Fast?Query? Evalua)on.?Proc. ACM-SIGIR 2002.? ?? Variable?byte?codes? ?? V.?N.?Anh?and?A.?Mo?at.?2005.?Inverted?Index? Compression?Using?Word-Aligned?Binary?Codes.? Informa)on Retrieval 8:?151–166.??? ?? Word?aligned?codes?
Ch. 5 
48?