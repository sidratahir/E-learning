from sklearn.feature_extraction.text import CountVectorizer
from sklearn.feature_extraction.text import TfidfTransformer
from numpy.linalg import norm
from scipy import dot
from nltk.tokenize import RegexpTokenizer
from nltk.stem import PorterStemmer

import os
import csv
from nltk.corpus import stopwords

import string

'''Variables'''
tokenizer = RegexpTokenizer(r'\w+')
stemmer = PorterStemmer()
path ="E:/ir/"
docs = []
sum = 0
def count(word, blob):
    return blob.words.count(word)
'''Setmming for changing verbs to base form'''
def stem_tokens(tokens, stemmer):
    stemmed = []
    for item in tokens:
        stemmed.append(stemmer.stem(item))
    return stemmed


'''Converting Sentences to tokens'''
def tokenize(text):

    tokens = tokenizer.tokenize(text)
    punctuations = ['(', ')', ';', ':', '[', ']', ',', '?', '!', '¥', '--', '/', '&']
    stop_words = stopwords.words('english')
    keywords = [word for word in tokens if
                word.isalpha() and not word in stop_words and not word in string.punctuation and not word.isdigit()]
    stems = stem_tokens(keywords, stemmer)
    return stems



'''File Reading'''
for subdir, dirs, files in os.walk(path):
    for file in files:
        file_path = subdir  + file
        shakes = open(file_path, 'r')
        text = shakes.read()
        docs.append(text)
        '''print(docs) '''

'''printing Words in documents'''
abc= sorted(tokenize(text))
print ('abc',abc)
newData = dict.fromkeys(abc).keys()
myFile = open('E:/toc.csv', 'w')
with myFile:
    writer = csv.writer(myFile)
    writer.writerow(newData)
